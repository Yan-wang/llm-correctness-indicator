# Experiment Summary: Evolving LLM Confidence Signatures

## Overview
This experiment aimed to evolve a robust mathematical formula (a "confidence signature") to predict the correctness of LLM-generated traces. These signatures are used as weights in a majority voting system to improve collective answer accuracy on complex math problems (AIME and HMMT).

## 1. Experiment Setup
*   **Model:** Qwen3-8B-AWQ traces.
*   **Dataset:** 13 balanced math problems (AIME/HMMT).
    *   **Data Points:** Each problem generates **10 test cases** (data points).
    *   **Case Composition:** Each test case includes **20 traces** sampled from a pool of 10 (8 Correct : 12 Incorrect, a 4:6 ratio).
    *   **Train Set:** 9 problems (**90 test cases total**).
    *   **Validation Set:** 4 problems (**40 test cases total**, held-out from training to test generalization).
*   **Task:** Evolve the `calculate_confidence(logprobs)` function to return a scalar weight.
*   **Voting Logic:** Weighted Majority Vote using the evolved scores as weights for each trace's answer.

## 2. Iterative Improvements & Trials

### Phase 1: The Discrete Plateau
*   **Trial:** Initially optimized for discrete **Voting Accuracy** (0/1 per case).
*   **Issue:** The evolution got stuck. Because accuracy is a sparse metric, small improvements in weight distribution didn't change the final accuracy, leaving the LLM with no "gradient" to follow. 
*   **Theoretical Note:** While a discrete metric can simulate a continuous reward if given an **enormous amount of data points** (where the quantization becomes fine enough to act as a gradient), this is impractical for LLM evolution due to computational costs.
*   **Improvement:** Implemented a **"Smooth" Reward**. In the presence of sparse data (90-130 cases), a continuous reward is a necessary workaround to reveal the internal "near-misses" and guide the evolution through subtle improvements.

### Phase 2: Selecting the Optimal Proxy Metric
*   **Trial:** We explored several continuous proxy metrics to guide the evolution: **MSE** (Mean Squared Error), **WSR** (Weighted Separation Ratio), **Voting Margin**, and **Softmax Probability**.
*   **Correlation Analysis:** We investigated the correlation between these proxies and the final Voting Accuracy.
*   **Finding:** Softmax Probability was found to be the most effective signal. Unlike per-trace metrics (like MSE or WSR), which optimize individual weights in isolation, Softmax Probability captures the **collective dynamics** of the group vote. It correctly penalizes cases where many wrong answers agree, even if their individual trace scores are moderately low.
*   **Formula Used:**
    \[ \text{Reward}_{\text{case}} = \frac{e^{W_{\text{correct}}}}{\sum_{i} e^{W_i}} \]
    *Where \(W_i\) is the sum of weights for the \(i\)-th unique answer in the test case.*

### Phase 3: Overfitting & Hard Cases
*   **Trial:** Using simple random sampling of traces.
*   **Issue:** The model easily reached 100% accuracy on easy problems but failed miserably on unseen hard problems (Validation accuracy ~40-50%).
*   **Improvement:** 
    *   **Problem-Level Split:** Isolated 4 specific problems for validation only, ensuring the LLM never saw their traces during evolution.
    *   **Underdog Challenge:** Forced a 4:6 Correct-to-Incorrect ratio in training. This made the task harder, forcing the formula to actively identify and suppress incorrect traces rather than relying on a correct-majority bias.

### Phase 4: Prompt Engineering & Feedback
*   **Trial:** Standard "Improve this code" prompts.
*   **Issue:** The LLM kept trying variations of the same `mean(logprobs)` formula without identifying new predictive dimensions.
*   **Improvement:** 
    *   **Failure Artifacts:** The evaluator now provides the **Top 5 most confident failures**, showing exactly where the formula was "tricked."
    *   **Concept Seeding:** Encouraged the LLM to explore "Temporal Disparity," "Logprob Volatility," and "The Glide" (end-of-trace decay).

### Phase 5: Decoupling Feature Discovery from Parameter Tuning
*   **Trial:** We introduced a hybrid architecture where the LLM is responsible only for discovering **predictive mathematical features**, while a numerical optimizer (L-BFGS-B) finds the optimal weights.
*   **Optimization Goal:** The evaluator now uses `scipy.optimize.minimize` to maximize the **Collective Softmax Probability** of correct answers across all training problems.
*   **Data Integrity:** To prevent data leakage, we removed validation set failures from the LLM's feedback loop, providing only training set failures and the results of the weight optimization (`optimized_weights`).
*   **Finding:** This specialization allows the LLM to act as a "Feature Scientist," focusing on creative signal discovery, while leaving the high-precision numerical tuning to a specialized algorithm.

The experiment successfully broke the baseline plateau through several stages of evolution.

### Stage 1: Manual Feature & Weight Evolution (Phases 1-4)
In these phases, the LLM was responsible for both the mathematical logic and the manual assignment of weights/thresholds.

| Metric | Initial Baseline | Phase 1-4 Peak (Iter 79) | Improvement |
| :--- | :--- | :--- | :--- |
| **Train Voting Acc** | 73.3% | 75.5% | +2.2% |
| **Val Voting Acc** | 57.5% | **82.5%** | **+25.0%** |
| **Smooth Reward** | 0.724 | 0.751 | +0.027 |

#### Analysis of Phase 1-4 Breakthrough
*   **Generalization:** The formula evolved in the final iterations (71-79) proved to be exceptionally robust, seeing a massive **25% jump in validation accuracy** while training accuracy remained stable.
*   **Robustness:** The evolved signature successfully suppressed incorrect "hallucination ruts" that standard average logprobs could not detect.

### Stage 2: Decoupled Feature Discovery (Phase 5)
By offloading numerical optimization to L-BFGS-B, we achieved a significant jump in both accuracy and speed. Note that the **Decoupled Initial** (Iter 0) already outperforms the Stage 1 manual baseline because the optimizer finds better weights for the same initial features.

| Metric | Phase 1-4 Peak | Decoupled Initial (Iter 0) | Decoupled Peak (Iter 30) | Improvement (vs Manual Peak) |
| :--- | :--- | :--- | :--- | :--- |
| **Train Voting Acc** | 75.5% | 64.4% | **94.4%** | **+18.9%** |
| **Val Voting Acc** | 82.5% | 72.5% | **87.5%** | **+5.0%** |
| **Smooth Reward** | 0.751 | 0.594 | **0.936** | **+0.185** |

#### Analysis of the Phase 5 Breakthrough
The shift to a decoupled architecture (LLM for features, L-BFGS-B for weights) produced a dramatic shift in both **speed** and **ceiling**:

1.  **Quicker Evolution:** While Phase 4 took ~80 iterations to find a meaningful signal, Phase 5 reached a 90%+ training reward in **under 10 iterations**. By offloading the numerical "brute force" to a specialized optimizer, the LLM was freed to focus purely on high-level mathematical intuition.
2.  **Higher Precision:** LLMs struggle with precise floating-point weights (e.g., "use 0.43 for feat_A and -0.12 for feat_B"). The L-BFGS-B optimizer finds the *mathematically perfect* balance for any feature set the LLM proposes, ensuring that even subtle signals are utilized to their full potential.
3.  **Enhanced Generalization:** By achieving 87.5% validation accuracy (Iteration 30), the method demonstrated that combining 3-4 simple mathematical primitives (like `bottom_10_window_mean` and `token_conf_std`) is far more robust than the complex, nested if-else logic the LLM previously attempted.

## 4. Open Questions & Future Research

### 1. Is a validation set needed for Early Stopping?
*   **Analysis:** In LLM-driven evolution, overfitting is not just numerical but "semantic." The LLM can interpret training failures and propose logic that specifically targets noise in the training set (e.g., "if trace length is exactly 142 tokens..."). 
*   **Thought:** Yes, integrating a validation-based early stopping mechanism is a high priority. While our current results show excellent generalization, as we increase the number of iterations or the model's capacity, the risk of "memorizing" the training problems increases. Stopping when the **Validation Smooth Reward** plateaus or diverges would be the most robust control logic.

### 2. Sensitivity to Group Size
*   **Analysis:** Our optimization objective (Softmax Probability) is a group-wise operator. The weights are numerically tuned to ensure the *sum* of correct weights outweighs the *sum* of incorrect ones in an ensemble of 20 traces.
*   **Thought:** The formula is likely sensitive to $N$ (the number of traces per vote). A formula optimized for $N=20$ might be too "spiky" for $N=5$, where a single highly-confident wrong trace could more easily flip the result. Future work should test the "Scale Invariance" of the evolved signatures across different ensemble sizes.

### 3. Does the absolute Confidence Score mean anything?
*   **Analysis:** Currently, the "score" is a linear combination of scaled features. Its magnitude is relative.
*   **Thought:** While the current scores are effective for **ranking** traces within a group, they are not yet "calibrated probabilities." A score of `1.5` doesn't inherently mean "90% chance of being correct." To make the score generalizable as a standalone threshold (e.g., "reject all traces with score < 0.5"), we would need to apply **Probability Calibration** (like Platt Scaling) on a hold-out set after the weights are fixed. This would turn the evolved signature into a true "Self-Correction" signal that works even for single-trace inference.

### 4. Single-Feature vs. Multi-Feature Architectures
*   **Analysis:** We shifted from evolving a single scalar score to evolving a "Feature Set" that is linearly combined by an optimizer. 
*   **Thought:** 
    *   **Simplicity/Explainability:** A single-feature approach (e.g., "Average Logprob") is the ultimate "white box"â€”you can explain the confidence to a user with one number. However, correctness in LLMs is often multi-faceted. 
    *   **Power:** Multi-feature architectures allow the evolution to discover **compensatory logic** (e.g., "High confidence is good, but *only if* the standard deviation is low"). 
    *   **Recommendation:** While multi-feature is more powerful, we should aim for **Sparsity**. A model with 3-4 highly interpretable features (e.g., `min_conf`, `volatility`, `end_decay`) combined linearly is the "sweet spot" between a black-box neural network and an over-simplistic single primitive.

## 5. Conclusion
Evolving confidence signatures requires moving beyond discrete metrics. The combination of **continuous feedback (Smooth Reward)**, **adversarial training data (4:6 ratio)**, and **forced analytical reasoning** allowed the evolution to discover generalizable mathematical properties of correctness that significantly outperform standard statistical baselines.

