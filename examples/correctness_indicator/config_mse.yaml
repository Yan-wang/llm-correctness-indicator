# Configuration for correctness indicator example
max_iterations: 10
checkpoint_interval: 5

# LLM configuration
llm:
  models:
    - name: "deepseek-chat"
      weight: 1.0
  api_base: "https://api.deepseek.com"
  api_key: ${OPENAI_API_KEY}
  temperature: 0.7
  max_tokens: 8192
  timeout: 120

# Prompt configuration
prompt:
  system_message: |
    You are an expert programmer specializing in correctness indicators for LLM-generated solutions. Your task is to evolve and improve a confidence score calculation algorithm that analyzes logprobs from LLM traces to determine answer confidence.
    
    EVALUATION METRIC:
    Your confidence scoring algorithm will be evaluated using the negative Mean Squared Error (MSE) score implemented in examples/correctness_indicator/evaluator.py. The evaluate() function in that file will:
    1. Call your compute_confidence_score() function to get a list of (confidence_score, is_correct) tuples
    2. Compute the Score using compute_mse_from_scores()
    
    MSE DEFINITION:
    The MSE metric measures the average squared difference between your confidence scores and the actual correctness labels (1 for correct, -1 for incorrect). Higher Score (closer to 0) means better performance.
    
    Formula:
        Score = -MSE = -1/N * Σ (confidence_score_i - correctness_label_i)²
    
    Where:
        N = total number of traces
        confidence_score_i = score assigned to trace i
        correctness_label_i = 1 if trace i is correct, -1 otherwise
    
    IMPLEMENTATION (from evaluator.py):
        scores = np.array([score for score, _ in confidence_scores])
        labels = np.array([1 if is_correct else -1 for _, is_correct in confidence_scores])
        # Note: Higher score is better, so we return negative MSE
        mse = np.mean((scores - labels)**2)
        score = -mse
    
    OPTIMIZATION GOAL:
    Your objective is to maximize the Score (minimize MSE). This means your confidence scores should:
    - Be as close to 1.0 as possible for correct answers
    - Be as close to -1.0 as possible for incorrect answers
    - You may need to normalize or scale your confidence calculations to fit this [-1, 1] range.
    
    This will enable weighted majority voting to produce more accurate final answers by giving more weight to high-confidence (and thus more likely correct) traces.
    
    CONSTRAINTS:
    - Each trace contains logprobs for the entire solution
    - Your compute_confidence_score() function must return List[Tuple[float, bool]] where each tuple is (confidence_score, is_correct)
    - The function should accept an optional input_file parameter (supports wildcard patterns like 'aime_traces/1_*_labeled.jsonl')

# Database configuration
database:
  population_size: 50
  archive_size: 20
  num_islands: 3
  elite_selection_ratio: 0.2
  exploitation_ratio: 0.7

  # embedding_model: "text-embedding-3-small"
  similarity_threshold: 0.99

# Evaluator configuration
evaluator:
  timeout: 60
  cascade_evaluation: false
  cascade_thresholds: [1.3]
  parallel_evaluations: 3
  validation_file: "./data/20260111_043209_traces_Qwen3-8B-AWQ_kv_auto_t0.6_p0.95_max32000_n64_swap4/val_split.jsonl"

# Evolution settings
diff_based_evolution: true
max_code_length: 20000
